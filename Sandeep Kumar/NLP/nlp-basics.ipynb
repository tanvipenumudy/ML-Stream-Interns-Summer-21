{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "import nltk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "nltk.download()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "sys.prefix"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'E:\\\\Programs\\\\Anaconda'"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "from nltk.corpus import brown"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "print(brown.categories())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "data=brown.sents(categories=\"humor\")\r\n",
    "len(data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1053"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "' '.join(data[8])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The robe , however , was missing , for by that time Barco had disposed of it at a pawnshop in Glendale .'"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bag of Words Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "document=\"\"\"As he crossed toward the pharmacy at the corner he involuntarily turned his head \r\n",
    "because of a burst of light that had ricocheted from his temple, and saw, with that quick smile with which we greet \r\n",
    "a rainbow or a rose. A blindingly white parallelogram of sky being unloaded from the van—a dresser \r\n",
    "with mirrors across which, as across a cinema screen, passed a flawlessly clear reflection of boughs sliding and swaying not arboreally. A human vacillation,\r\n",
    " produced by the nature of those who were carrying this sky, these boughs, this gliding façade.\"\"\"\r\n",
    "\r\n",
    "sentence=\"\"\"As he crossed toward the pharmacy at the corner he involuntarily turned his head because of a burst of light that had ricocheted from his temple, and saw, with that quick smile with which we greet a rainbow or a rose.\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "sents=sent_tokenize(document)\r\n",
    "print(sents)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['As he crossed toward the pharmacy at the corner he involuntarily turned his head \\nbecause of a burst of light that had ricocheted from his temple, and saw, with that quick smile with which we greet \\na rainbow or a rose.', 'A blindingly white parallelogram of sky being unloaded from the van—a dresser \\nwith mirrors across which, as across a cinema screen, passed a flawlessly clear reflection of boughs sliding and swaying not arboreally.', 'A human vacillation,\\n produced by the nature of those who were carrying this sky, these boughs, this gliding façade.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "print(word_tokenize(sentence))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['As', 'he', 'crossed', 'toward', 'the', 'pharmacy', 'at', 'the', 'corner', 'he', 'involuntarily', 'turned', 'his', 'head', 'because', 'of', 'a', 'burst', 'of', 'light', 'that', 'had', 'ricocheted', 'from', 'his', 'temple', ',', 'and', 'saw', ',', 'with', 'that', 'quick', 'smile', 'with', 'which', 'we', 'greet', 'a', 'rainbow', 'or', 'a', 'rose', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "sw=set(stopwords.words('english'))\r\n",
    "print(sw)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'their', \"mightn't\", 'while', 'itself', 'o', 'in', 'him', 'theirs', 'on', 'a', 'with', 'there', \"haven't\", 'which', 'from', 'no', 'through', 'and', 'until', 'll', 'won', \"it's\", 'is', \"doesn't\", \"shouldn't\", 'needn', 'should', 'each', 'some', 'ain', \"hasn't\", \"isn't\", 'same', 'wouldn', 'has', 'other', 'but', 'd', 'y', \"hadn't\", 'under', 'after', 'be', 'have', 'he', 'those', 'during', 'any', 'm', 'as', 'himself', 'between', 'your', 'than', 'am', 'me', 'whom', 'them', 'nor', 'more', 'had', 'does', \"mustn't\", 'few', \"should've\", 'because', 'shouldn', 'haven', 'myself', \"you're\", 'we', 'yourself', 'my', 'ourselves', 'can', \"shan't\", 'having', 'further', 'ours', 'his', 'very', 'to', 'too', 'just', \"don't\", 'do', 'being', 'so', 've', 'here', 'didn', 'mustn', 'all', 'above', 'what', 'was', 'most', \"you'd\", 'down', 'been', 'doesn', 'below', \"you've\", 't', 'these', 'she', 'if', 'out', 'weren', 're', \"she's\", 'themselves', 'this', 'about', 'are', 'will', 'not', 'before', 'of', \"aren't\", 'once', 'did', 'for', \"you'll\", 'both', 'yourselves', \"didn't\", 'into', 'or', \"needn't\", 'i', 'by', 'such', 's', 'ma', 'only', \"wasn't\", 'how', 'doing', 'they', 'yours', 'shan', 'who', 'when', 'hasn', 'now', 'the', 'off', 'our', 'were', 'own', 'herself', 'at', 'up', 'that', \"that'll\", 'then', \"wouldn't\", 'don', 'again', 'hers', 'hadn', 'you', 'aren', 'its', 'over', 'couldn', 'it', 'an', 'mightn', \"won't\", 'against', 'her', \"couldn't\", 'isn', \"weren't\", 'wasn', 'where', 'why'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "def remove_stopwords(text,stopwords):\r\n",
    "    useful_words=[w for w in text if w not in stopwords]\r\n",
    "    return useful_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "print(sentence)\r\n",
    "text=remove_stopwords(word_tokenize(sentence),sw)\r\n",
    "print(text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "As he crossed toward the pharmacy at the corner he involuntarily turned his head because of a burst of light that had ricocheted from his temple, and saw, with that quick smile with which we greet a rainbow or a rose.\n",
      "['As', 'crossed', 'toward', 'pharmacy', 'corner', 'involuntarily', 'turned', 'head', 'burst', 'light', 'ricocheted', 'temple', ',', 'saw', ',', 'quick', 'smile', 'greet', 'rainbow', 'rose', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regexp Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "sentence=\"Send me pictures of chapter 1,2,3 notes\"\r\n",
    "tokenizer=RegexpTokenizer('[a-zA-Z]+')\r\n",
    "usefultext=tokenizer.tokenize(sentence)\r\n",
    "usefultext"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Send', 'me', 'pictures', 'of', 'chapter', 'notes']"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stemming\r\n",
    "\r\n",
    "Process that transforms verbs, radicals to their radical forms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "text=\"\"\"Foxes love to make jumps. The quick brown fox was seen jumping over the\r\n",
    "        lovely dog from a 6ft high wall\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\r\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "ss=SnowballStemmer('english')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "ss.stem('university')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "ss.stem('universe')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "from nltk.stem import WordNetLemmatizer\r\n",
    "wn=WordNetLemmatizer()\r\n",
    "wn.lemmatize('Universe')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Universe'"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "wn.lemmatize('university')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'university'"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building a Vocab and Vectorization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "# Sample Corpus - Contains 4 Documents, each document can have 1 or more sentences\r\n",
    "corpus = [\r\n",
    "        'Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\r\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM',\r\n",
    "        'The nobel laurate won the hearts of the people.',\r\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story.'\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "cv = CountVectorizer()\r\n",
    "vectorized_corpus=cv.fit_transform(corpus)\r\n",
    "vectorized_corpus=vectorized_corpus.toarray()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "vectorized_corpus"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 0, 2],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "print(vectorized_corpus[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
      " 2 0 1 0 2]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "print(len(vectorized_corpus[0]),len(cv.vocabulary_.keys())) # Both are same"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "42 42\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "# Reverse Mapping\r\n",
    "numbers =vectorized_corpus[2]\r\n",
    "numbers"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "s=cv.inverse_transform(numbers)\r\n",
    "print(s)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array(['hearts', 'laurate', 'nobel', 'of', 'people', 'the', 'won'],\n",
      "      dtype='<U9')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vectorization with Stopword removal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "def mytokenizer(document):\r\n",
    "    words=tokenizer.tokenize(document.lower())\r\n",
    "    # Removing the stopwords\r\n",
    "    words=remove_stopwords(words,sw)\r\n",
    "    return words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "sentence"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Send me pictures of chapter 1,2,3 notes'"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "mytokenizer(sentence)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['send', 'pictures', 'chapter', 'notes']"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "cv=CountVectorizer(tokenizer=mytokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "vectorized_corpus=cv.fit_transform(corpus).toarray()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "print(vectorized_corpus)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 1 0 1 2 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 2]\n",
      " [0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "cv.inverse_transform(vectorized_corpus)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array(['capt', 'cricket', 'cup', 'held', 'indian', 'kohli', 'lanka',\n",
       "        'says', 'sri', 'team', 'virat', 'wins', 'world'], dtype='<U9'),\n",
       " array(['confident', 'elections', 'indian', 'lok', 'next', 'pm', 'sabha',\n",
       "        'says', 'win'], dtype='<U9'),\n",
       " array(['hearts', 'laurate', 'nobel', 'people'], dtype='<U9'),\n",
       " array(['based', 'exciting', 'indian', 'movie', 'raazi', 'real', 'spy',\n",
       "        'story', 'thriller', 'upon'], dtype='<U9')]"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "# For Test Data\r\n",
    "test_corpus=[\r\n",
    "    'Indian cricket is good!',\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We do fit_transform on the training data and transform on the test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "vectorized_test_corpus=cv.transform(test_corpus).toarray()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "cv.inverse_transform(vectorized_test_corpus)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array(['cricket', 'indian'], dtype='<U9')]"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Unigram\r\n",
    "* bigrams\r\n",
    "* Trigrams\r\n",
    "* n-grams(mix of above three)\r\n",
    "* TF-IDF Normalisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "sent1= [\"This is good movie\"]\r\n",
    "sent2=[\"this is good movie but actor is not present\"]\r\n",
    "sent3=[\"this is not a good movie\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\r\n",
    "cv=CountVectorizer(ngram_range=(2,2))\r\n",
    "docs=[sent1[0],sent2[0]]\r\n",
    "cv.fit_transform(docs).toarray()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "cv.vocabulary_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'this is': 7,\n",
       " 'is good': 3,\n",
       " 'good movie': 2,\n",
       " 'movie but': 5,\n",
       " 'but actor': 1,\n",
       " 'actor is': 0,\n",
       " 'is not': 4,\n",
       " 'not present': 6}"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tf-idf Normalisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "sent1=\"this is good movie\"\r\n",
    "sent2 =\"this was good movie\"\r\n",
    "sent3=\"this is not good movie\"\r\n",
    "\r\n",
    "corpus = [sent1,sent2,sent3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "tfidf = TfidfVectorizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "vc=tfidf.fit_transform(corpus).toarray()\r\n",
    "vc"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.46333427, 0.59662724, 0.46333427, 0.        , 0.46333427,\n",
       "        0.        ],\n",
       "       [0.41285857, 0.        , 0.41285857, 0.        , 0.41285857,\n",
       "        0.69903033],\n",
       "       [0.3645444 , 0.46941728, 0.3645444 , 0.61722732, 0.3645444 ,\n",
       "        0.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "print(vc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.46333427 0.59662724 0.46333427 0.         0.46333427 0.        ]\n",
      " [0.41285857 0.         0.41285857 0.         0.41285857 0.69903033]\n",
      " [0.3645444  0.46941728 0.3645444  0.61722732 0.3645444  0.        ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "tfidf.vocabulary_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}"
      ]
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92c53777c7e45b4084d16b9941c1ff1c739a249f1654c0159592f90e5d117398"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}