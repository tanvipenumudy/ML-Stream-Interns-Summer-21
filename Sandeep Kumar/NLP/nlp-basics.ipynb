{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import nltk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "nltk.download()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from nltk.corpus import brown"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(brown.categories())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "data=brown.sents(categories=\"humor\")\r\n",
    "len(data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1053"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "' '.join(data[8])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The robe , however , was missing , for by that time Barco had disposed of it at a pawnshop in Glendale .'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bag of Words Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "document=\"\"\"As he crossed toward the pharmacy at the corner he involuntarily turned his head \r\n",
    "because of a burst of light that had ricocheted from his temple, and saw, with that quick smile with which we greet \r\n",
    "a rainbow or a rose. A blindingly white parallelogram of sky being unloaded from the van—a dresser \r\n",
    "with mirrors across which, as across a cinema screen, passed a flawlessly clear reflection of boughs sliding and swaying not arboreally. A human vacillation,\r\n",
    " produced by the nature of those who were carrying this sky, these boughs, this gliding façade.\"\"\"\r\n",
    "\r\n",
    "sentence=\"\"\"As he crossed toward the pharmacy at the corner he involuntarily turned his head because of a burst of light that had ricocheted from his temple, and saw, with that quick smile with which we greet a rainbow or a rose.\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "sents=sent_tokenize(document)\r\n",
    "print(sents)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['As he crossed toward the pharmacy at the corner he involuntarily turned his head \\nbecause of a burst of light that had ricocheted from his temple, and saw, with that quick smile with which we greet \\na rainbow or a rose.', 'A blindingly white parallelogram of sky being unloaded from the van—a dresser \\nwith mirrors across which, as across a cinema screen, passed a flawlessly clear reflection of boughs sliding and swaying not arboreally.', 'A human vacillation,\\n produced by the nature of those who were carrying this sky, these boughs, this gliding façade.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "print(word_tokenize(sentence))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['As', 'he', 'crossed', 'toward', 'the', 'pharmacy', 'at', 'the', 'corner', 'he', 'involuntarily', 'turned', 'his', 'head', 'because', 'of', 'a', 'burst', 'of', 'light', 'that', 'had', 'ricocheted', 'from', 'his', 'temple', ',', 'and', 'saw', ',', 'with', 'that', 'quick', 'smile', 'with', 'which', 'we', 'greet', 'a', 'rainbow', 'or', 'a', 'rose', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "sw=set(stopwords.words('english'))\r\n",
    "print(sw)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'here', 'through', 'both', 'were', 'in', 'all', 'not', \"should've\", 'no', \"mightn't\", 'his', 'do', \"needn't\", 'ma', 'should', 'during', 'will', 'by', 'whom', 'for', \"don't\", 'up', 'only', 'just', 'as', 'didn', \"wasn't\", 'did', 'there', 've', 'once', 'about', 'yourselves', 'aren', 'down', \"weren't\", 'but', 'on', \"you're\", 'before', \"shan't\", 'have', 'to', 'under', 'over', 'ain', 'she', 'itself', 'of', \"hasn't\", 'after', 'with', 'who', 's', 'the', 'those', 'and', 'while', 'wasn', 'isn', 'at', 'or', 'can', 'wouldn', 'because', 'yours', 'they', 'doesn', \"you'd\", 'such', 'ourselves', 'from', 'her', 'a', 'same', 'having', 'we', 'why', 'them', 'above', 'don', 'that', 'm', \"didn't\", 'other', \"doesn't\", 'our', \"she's\", 'myself', 'are', \"you'll\", 'it', 'i', 'him', 'herself', 'being', 'off', 'so', 'weren', 'you', 'won', 'few', 'then', 'its', 'where', 'now', 'hadn', \"shouldn't\", 'own', 'shouldn', 'had', 'out', 'some', 'he', 'any', 'am', 'my', \"won't\", 'himself', 'was', 're', \"isn't\", 'against', 'too', \"that'll\", 'o', 'y', 'hasn', 'hers', 'doing', 'be', 'nor', 'when', 'further', 'has', \"aren't\", \"you've\", 'again', 'mightn', 'each', 'shan', 'been', 'very', 'mustn', 'd', 'haven', 'than', \"mustn't\", 'what', 'does', 'll', 'an', 'needn', 'this', 'their', 'is', 'theirs', 'until', \"it's\", 'if', 'most', \"couldn't\", 'between', 'these', \"haven't\", \"hadn't\", 'how', 'your', 'ours', 'which', 't', 'below', \"wouldn't\", 'themselves', 'more', 'couldn', 'me', 'yourself', 'into'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def remove_stopwords(text,stopwords):\r\n",
    "    useful_words=[w for w in text if w not in stopwords]\r\n",
    "    return useful_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "print(sentence)\r\n",
    "text=remove_stopwords(word_tokenize(sentence),sw)\r\n",
    "print(text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "As he crossed toward the pharmacy at the corner he involuntarily turned his head because of a burst of light that had ricocheted from his temple, and saw, with that quick smile with which we greet a rainbow or a rose.\n",
      "['As', 'crossed', 'toward', 'pharmacy', 'corner', 'involuntarily', 'turned', 'head', 'burst', 'light', 'ricocheted', 'temple', ',', 'saw', ',', 'quick', 'smile', 'greet', 'rainbow', 'rose', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regexp Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "sentence=\"Send me pictures of chapter 1,2,3 notes\"\r\n",
    "tokenizer=RegexpTokenizer('[a-zA-Z]+')\r\n",
    "usefultext=tokenizer.tokenize(sentence)\r\n",
    "usefultext"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Send', 'me', 'pictures', 'of', 'chapter', 'notes']"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stemming\r\n",
    "\r\n",
    "Process that transforms verbs, radicals to their radical forms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "text=\"\"\"Foxes love to make jumps. The quick brown fox was seen jumping over the\r\n",
    "        lovely dog from a 6ft high wall\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\r\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "ss=SnowballStemmer('english')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "ss.stem('university')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "ss.stem('universe')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "from nltk.stem import WordNetLemmatizer\r\n",
    "wn=WordNetLemmatizer()\r\n",
    "wn.lemmatize('Universe')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Universe'"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "wn.lemmatize('university')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'university'"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building a Vocab and Vectorization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "92c53777c7e45b4084d16b9941c1ff1c739a249f1654c0159592f90e5d117398"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}